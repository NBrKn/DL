{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular Neural Network\n",
    "\n",
    "_Objective_: To build a modular neural network program for experimentation with mimimum use of libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_References:_\n",
    "\n",
    "Overall base: deeplearning.ai specialization, Andrew Ng, Coursera  \n",
    "[CS231n](cs231n.github.io), Andrej Karpathy   \n",
    "[Stats385](stats385.github.io)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "#import dill as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Kaggle MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = df.as_matrix()\n",
    "train_y = train[:,0].astype('int8')\n",
    "train_x = train[:,1:].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 42000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 42000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = train_x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 42000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.reshape(42000).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ..., 7 6 9]\n"
     ]
    }
   ],
   "source": [
    "for i in train_y:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp, axis = 1, keepdims=True )\n",
    "    s = x_exp/x_sum\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x*(x>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_d(x):\n",
    "    return 1*(x>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init Params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_zeros(layers_dims):    \n",
    "    parameters = {}\n",
    "    L = len(layers_dims)                \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l - 1]))\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_random(layers_dims):\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * 10\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_he(layers_dims):\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * np.sqrt(2 / layers_dims[l - 1])\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_params(layers_dims, md=\"he\"):\n",
    "    if md==\"he\":\n",
    "        return initialize_parameters_he(layers_dims)\n",
    "    elif md==\"random\":\n",
    "        return initialize_parameters_random(layers_dims)\n",
    "    elif md==\"zeros\":\n",
    "        return initialize_parameters_zeros(layers_dims)\n",
    "    else:\n",
    "        return ValueError(\"Enter mode as either he, random or zeros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam Optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "\n",
    "        s[\"dW\" + str(l+1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        s[\"db\" + str(l+1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,\n",
    "                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "  \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
    "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n",
    "        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
    "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s[\"dW\" + str(l + 1)] + epsilon)\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s[\"db\" + str(l + 1)] + epsilon)\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(a3, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n",
    "    cost = 1./m * np.sum(logprobs)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(a3, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n",
    "    cost = 1./m * np.sum(logprobs)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, optimizer, learning_rate=0.0007, init_mode=\"he\", reg=True, mini_batch_size=64, beta=0.9,\n",
    "          beta1=0.9, beta2=0.999, epsilon=1e-8, num_epochs=1000, print_cost=True):\n",
    "\n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = init_params(layers_dims, init_mode)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        #seed = seed + 1\n",
    "        #minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "\n",
    "        #for minibatch in minibatches:\n",
    "\n",
    "        # Select a minibatch\n",
    "        #(minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "        # Forward propagation\n",
    "        a3, caches = forward_propagation(X, parameters)\n",
    "\n",
    "        # Compute cost\n",
    "        \n",
    "        cost = compute_cost(a3, Y)\n",
    "\n",
    "        \n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(X, Y, caches)\n",
    "\n",
    "    \n",
    "    \n",
    "       # Update parameters\n",
    "       \n",
    "        if optimizer == \"gd\":\n",
    "            parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "        elif optimizer == \"momentum\":\n",
    "             parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "        elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        \n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after epoch %i: %f\" % (i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(costs)\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('epochs (per 100)')\n",
    "        plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "    return parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
